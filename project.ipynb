{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.55\n",
      "Will train until validation_0-merror hasn't improved in 100 rounds.\n",
      "[1]\tvalidation_0-merror:0.528\n",
      "[2]\tvalidation_0-merror:0.52\n",
      "[3]\tvalidation_0-merror:0.516\n",
      "[4]\tvalidation_0-merror:0.522\n",
      "[5]\tvalidation_0-merror:0.536\n",
      "[6]\tvalidation_0-merror:0.526\n",
      "[7]\tvalidation_0-merror:0.522\n",
      "Elapsed training time:  0.9725926999999501\n",
      "Elapsed Predicting time:  0.01631760000009308\n",
      "[0.53       0.49333333 0.5        0.48       0.51      ]\n",
      "accuracy 0.484\n",
      "confusion_matrix:\n",
      "    0   1    2   3  4\n",
      "0  0   4   13   3  0\n",
      "1  0  10   67  10  1\n",
      "2  0  13  213  11  2\n",
      "3  2   6   93  17  2\n",
      "4  0   5   18   8  2\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        20\n",
      "           1       0.26      0.11      0.16        88\n",
      "           2       0.53      0.89      0.66       239\n",
      "           3       0.35      0.14      0.20       120\n",
      "           4       0.29      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.48       500\n",
      "   macro avg       0.28      0.24      0.22       500\n",
      "weighted avg       0.40      0.48      0.40       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"xgb.py: Copyright 2020, Sentiment Analysis on Movie Reviews using LSTM\"\"\"\n",
    "__authors__ = \"Xueru Ye, Ruoran Liu, Keith Herbert\"\n",
    "__copyright__ = \"Copyright 2020, Sentiment Analysis on Movie Reviews\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintained_by__ = \"Xueru Ye, Ruoran Liu, Keith Herbert\"\n",
    "__email__ = \"xye85@uwo.ca@uwo.ca, rliu454@uwo.ca, kherbe@uwo.ca\"\n",
    "__status__ = \"Production\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from memory_profiler import profile\n",
    "from time import perf_counter\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_train = pd.read_csv('data.tsv', delimiter='\\t')\n",
    "df_train = df_train.sample(n = 2000)\n",
    "df_train.fillna('null',inplace=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train['Phrase'], df_train['Sentiment'], test_size=0.25)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "tf_idf_transformer = TfidfTransformer()\n",
    "tf_idf = tf_idf_transformer.fit_transform(vectorizer.fit_transform(x_train))\n",
    "x_train_weight = tf_idf.toarray()  # TF-IDF matrix for training set\n",
    "tf_idf = tf_idf_transformer.transform(vectorizer.transform(x_test))\n",
    "x_test_weight = tf_idf.toarray()  # TF-IDF matrix for testing set\n",
    "\n",
    "random_seed = 8\n",
    "kfold_scoring = 'accuracy'\n",
    "kfold_n_splits = 5\n",
    "kfold_result_output = \"%s KFold Validation: Mean %f (STD %f)\"\n",
    "model_name = 'XGBoost Model'\n",
    "\n",
    "xlf = xgb.XGBClassifier(max_depth=6,\n",
    "                learning_rate=0.51,\n",
    "                n_estimators=8,\n",
    "                silent=True,\n",
    "                objective='multi:softmax',\n",
    "                num_class = 5,\n",
    "                nthread=-1,\n",
    "                gamma=0,\n",
    "                min_child_weight=1,\n",
    "                max_delta_step=0,\n",
    "                subsample=0.85,\n",
    "                colsample_bytree=0.7,\n",
    "                colsample_bylevel=1,\n",
    "                reg_alpha=0,\n",
    "                reg_lambda=1,\n",
    "                scale_pos_weight=1,\n",
    "                seed=1440,\n",
    "                missing=None)\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "#@profile\n",
    "def fit_predict(xlf_temp):\n",
    "    t1_start = perf_counter()\n",
    "    xlf.fit(x_train_weight, y_train, eval_metric='merror', verbose=True, eval_set=[(x_test_weight,  y_test)], early_stopping_rounds=100)\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed training time: \", t1_stop - t1_start)\n",
    "    \n",
    "    t1_start = perf_counter()\n",
    "    y_pred = xlf.predict(x_test_weight, ntree_limit=xlf.best_ntree_limit)\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed Predicting time: \", t1_stop - t1_start)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "y_pred = fit_predict(xlf)\n",
    "\n",
    "print(cross_val_score(xlf, x_train_weight, y_train, cv=5))\n",
    "\n",
    "label_all = ['0', '1','2','3','4']\n",
    "confusion_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "df = pd.DataFrame(confusion_mat, columns=label_all)\n",
    "df.index = label_all\n",
    "print('accuracy', metrics.accuracy_score(y_test, y_pred))\n",
    "print('confusion_matrix:\\n', df)\n",
    "print('classification report:\\n', metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Reference - https://towardsdatascience.com/distilling-bert-how-to-achieve-bert-performance-using-logistic-regression-69a7fc14249d\n",
    "# Reference - https://huggingface.co/transformers/examples.html#glue\n",
    "# Reference - https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.526\n",
      "Will train until validation_0-merror hasn't improved in 100 rounds.\n",
      "[1]\tvalidation_0-merror:0.514\n",
      "[2]\tvalidation_0-merror:0.518\n",
      "[3]\tvalidation_0-merror:0.526\n",
      "[4]\tvalidation_0-merror:0.516\n",
      "[5]\tvalidation_0-merror:0.518\n",
      "[6]\tvalidation_0-merror:0.512\n",
      "[7]\tvalidation_0-merror:0.514\n",
      "Elapsed training time:  0.9649547999999868\n",
      "Elapsed Predicting time:  0.020792200000187222\n",
      "[0.54333333 0.53333333 0.53333333 0.53333333 0.50333333]\n",
      "accuracy 0.488\n",
      "confusion_matrix:\n",
      "    0   1    2   3  4\n",
      "0  0   3   19   3  1\n",
      "1  1  14   71   5  0\n",
      "2  2  11  222  11  0\n",
      "3  0   6   96   7  1\n",
      "4  0   2   21   3  1\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        26\n",
      "           1       0.39      0.15      0.22        91\n",
      "           2       0.52      0.90      0.66       246\n",
      "           3       0.24      0.06      0.10       110\n",
      "           4       0.33      0.04      0.07        27\n",
      "\n",
      "    accuracy                           0.49       500\n",
      "   macro avg       0.30      0.23      0.21       500\n",
      "weighted avg       0.40      0.49      0.39       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"xgb.py: Copyright 2020, Sentiment Analysis on Movie Reviews using LSTM\"\"\"\n",
    "__authors__ = \"Xueru Ye, Ruoran Liu, Keith Herbert\"\n",
    "__copyright__ = \"Copyright 2020, Sentiment Analysis on Movie Reviews\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintained_by__ = \"Xueru Ye, Ruoran Liu, Keith Herbert\"\n",
    "__email__ = \"xye85@uwo.ca@uwo.ca, rliu454@uwo.ca, kherbe@uwo.ca\"\n",
    "__status__ = \"Production\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from memory_profiler import profile\n",
    "from time import perf_counter\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_train = pd.read_csv('data.tsv', delimiter='\\t')\n",
    "df_train = df_train.sample(n = 2000)\n",
    "df_train.fillna('null',inplace=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train['Phrase'], df_train['Sentiment'], test_size=0.25)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "tf_idf_transformer = TfidfTransformer()\n",
    "tf_idf = tf_idf_transformer.fit_transform(vectorizer.fit_transform(x_train))\n",
    "x_train_weight = tf_idf.toarray()  # TF-IDF matrix for training set\n",
    "tf_idf = tf_idf_transformer.transform(vectorizer.transform(x_test))\n",
    "x_test_weight = tf_idf.toarray()  # TF-IDF matrix for testing set\n",
    "\n",
    "random_seed = 8\n",
    "kfold_scoring = 'accuracy'\n",
    "kfold_n_splits = 5\n",
    "kfold_result_output = \"%s KFold Validation: Mean %f (STD %f)\"\n",
    "model_name = 'XGBoost Model'\n",
    "\n",
    "xlf = xgb.XGBClassifier(max_depth=6,\n",
    "                learning_rate=0.51,\n",
    "                n_estimators=8,\n",
    "                silent=True,\n",
    "                objective='multi:softmax',\n",
    "                num_class = 5,\n",
    "                nthread=-1,\n",
    "                gamma=0,\n",
    "                min_child_weight=1,\n",
    "                max_delta_step=0,\n",
    "                subsample=0.85,\n",
    "                colsample_bytree=0.7,\n",
    "                colsample_bylevel=1,\n",
    "                reg_alpha=0,\n",
    "                reg_lambda=1,\n",
    "                scale_pos_weight=1,\n",
    "                seed=1440,\n",
    "                missing=None)\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "#@profile\n",
    "def fit_predict(xlf_temp):\n",
    "    t1_start = perf_counter()\n",
    "    xlf.fit(x_train_weight, y_train, eval_metric='merror', verbose=True, eval_set=[(x_test_weight,  y_test)], early_stopping_rounds=100)\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed training time: \", t1_stop - t1_start)\n",
    "    \n",
    "    t1_start = perf_counter()\n",
    "    y_pred = xlf.predict(x_test_weight, ntree_limit=xlf.best_ntree_limit)\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed Predicting time: \", t1_stop - t1_start)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "y_pred = fit_predict(xlf)\n",
    "\n",
    "print(cross_val_score(xlf, x_train_weight, y_train, cv=5))\n",
    "\n",
    "label_all = ['0', '1','2','3','4']\n",
    "confusion_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "df = pd.DataFrame(confusion_mat, columns=label_all)\n",
    "df.index = label_all\n",
    "print('accuracy', metrics.accuracy_score(y_test, y_pred))\n",
    "print('confusion_matrix:\\n', df)\n",
    "print('classification report:\\n', metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.ipynb_checkpoints', 'archive', 'assignment1', 'assignment2', 'data.tsv', 'dbl_hybrid.py', 'lstm.py', 'lstm_kfold.py', 'project.ipynb', 'Untitled.ipynb', 'xgb.py', '__pycache__']\n",
      "len(word_set) 4207\n",
      "len(word_to_int) 4207\n",
      "max_len 43\n",
      "[[ 116   49 1094 1750 1517  319 1402 2584 1094    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1591    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [4039 2199 3238 1737 4130 1750 2842 1401 1354 1237 1152    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(1600, 43) \n",
      "Validation set: \t(200, 43) \n",
      "Test set: \t\t(200, 43)\n",
      "160\n",
      "20\n",
      "20\n",
      "No GPU available, training on CPU.\n",
      "SentimentRNN(\n",
      "  (embedding): Embedding(4208, 80)\n",
      "  (lstm): LSTM(80, 50, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=50, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 1/3... Step: 100... Loss: 1.313844... Val Loss: 1.296787\n",
      "Epoch: 2/3... Step: 200... Loss: 1.400220... Val Loss: 1.288555\n",
      "Epoch: 2/3... Step: 300... Loss: 1.313019... Val Loss: 1.298030\n",
      "Epoch: 3/3... Step: 400... Loss: 0.924650... Val Loss: 1.284374\n",
      "Elapsed Training Time: 20.403142600000137\n",
      "Test loss: 1.351\n",
      "Test accuracy: 0.455\n",
      "Elapsed Predicting Time: 0.2829633000001195\n",
      "0.455\n",
      "[[ 0  0 10  0  0]\n",
      " [ 0  0 36  0  0]\n",
      " [ 0  0 91  0  0]\n",
      " [ 0  0 49  0  0]\n",
      " [ 0  0 14  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00        36\n",
      "           2       0.46      1.00      0.63        91\n",
      "           3       0.00      0.00      0.00        49\n",
      "           4       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.46       200\n",
      "   macro avg       0.09      0.20      0.13       200\n",
      "weighted avg       0.21      0.46      0.28       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"lstm.py: Copyright 2020, Sentiment Analysis on Movie Reviews using LSTM\"\"\"\n",
    "__authors__ = \"Xueru Ye, Ruoran Liu, Keith Herbert\"\n",
    "__copyright__ = \"Copyright 2020, Sentiment Analysis on Movie Reviews\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintained_by__ = \"Xueru Ye, Ruoran Liu, Keith Herbert\"\n",
    "__email__ = \"kherbe@uwo.ca, xye85@uwo.ca@uwo.ca, rliu454@uwo.ca\"\n",
    "__status__ = \"Production\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "\n",
    "import unicodedata, re, string\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from memory_profiler import profile\n",
    "from time import perf_counter\n",
    "\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(os.listdir(\"./\"))\n",
    "\n",
    "df_train = pd.read_csv('data.tsv', delimiter='\\t')\n",
    "df_train = df_train.sample(n = 2000)\n",
    "\n",
    "# =========== Pre-processing =========== #\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(\"\\d+\", \"\", word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "#    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "# =========\n",
    "\n",
    "# First step - tokenizing phrases\n",
    "\n",
    "### nltk.download() # If you get an error here, uncomment this line and download the missing package\n",
    "df_train['Words'] = df_train['Phrase'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Second step - passing through prep functions\n",
    "df_train['Words'] = df_train['Words'].apply(normalize)\n",
    "df_train['Words'].head()\n",
    "\n",
    "# Third step - creating a list of unique words to be used as dictionary for encoding\n",
    "word_set = set()\n",
    "for l in df_train['Words']:\n",
    "    for e in l:\n",
    "        word_set.add(e)\n",
    "\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_set, 1)}\n",
    "\n",
    "# Check if they are still the same lenght\n",
    "print('len(word_set)',len(word_set))\n",
    "print('len(word_to_int)',len(word_to_int))\n",
    "\n",
    "# Now the dict to tokenize each phrase\n",
    "df_train['Tokens'] = df_train['Words'].apply(lambda l: [word_to_int[word] for word in l])\n",
    "df_train['Tokens'].head()\n",
    "\n",
    "\n",
    "# Step four - get the len of longest phrase\n",
    "max_len = df_train['Tokens'].str.len().max()\n",
    "print('max_len',max_len)\n",
    "\n",
    "# Pad each phrase representation with zeroes, starting from the beginning of sequence\n",
    "# Will use a combined list of phrases as np array for further work. This is expected format for the Pytorch utils to be used later\n",
    "\n",
    "all_tokens = np.array([t for t in df_train['Tokens']])\n",
    "encoded_labels = np.array([l for l in df_train['Sentiment']])\n",
    "\n",
    "# Create blank rows\n",
    "features = np.zeros((len(all_tokens), max_len), dtype=int)\n",
    "# for each phrase, add zeros at the end\n",
    "for i, row in enumerate(all_tokens):\n",
    "    features[i, :len(row)] = row\n",
    "\n",
    "#print first 3 values of the feature matrix\n",
    "print(features[:3])\n",
    "\n",
    "# =======================================================\n",
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of  resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "\n",
    "# =======================================================\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 10\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Check the size of the loaders (how many batches inside)\n",
    "print(len(train_loader))\n",
    "print(len(valid_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "# =======================================================\n",
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "\n",
    "# =======================================================\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x.to(torch.int64))\n",
    "\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # transform lstm output to input size of linear layers\n",
    "        lstm_out = lstm_out.transpose(0, 1)\n",
    "        lstm_out = lstm_out[-1]\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# Instantiate the model w/ hyperparams\n",
    "\n",
    "# Override - Tuned\n",
    "# epochs = 3 \n",
    "# embedding_dim = 80\n",
    "# hidden_dim = 50\n",
    "# lr = 0.04\n",
    "\n",
    "# Override - Untuned\n",
    "# epochs = 3 \n",
    "# embedding_dim = 40\n",
    "# hidden_dim = 25\n",
    "# lr = 0.02\n",
    "\n",
    "vocab_size = len(word_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 5\n",
    "embedding_dim = 80\n",
    "hidden_dim = 50\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)\n",
    "\n",
    "# ========================================================================\n",
    "# loss and optimization functions\n",
    "lr=0.004\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# ========================================================================\n",
    "\n",
    "# training params\n",
    "epochs = 3  # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "print_every = 100\n",
    "clip = 5  # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if (train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "#@profile\n",
    "def time_complexity():\n",
    "    counter = 0\n",
    "    t1_start = perf_counter()\n",
    "    # ========================================================================\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "            if (train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, labels.to(torch.int64))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if (train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, labels.to(torch.int64))\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "\n",
    "\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed Training Time:\", t1_stop - t1_start)\n",
    "    t1_start = perf_counter()\n",
    "    # ========================================================================\n",
    "    # Get test data loss and accuracy\n",
    "    predict_list = []\n",
    "    label_list = []\n",
    "    test_losses = []  # track loss\n",
    "    num_correct = 0\n",
    "\n",
    "    # init hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    # iterate over test data\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # get predicted outputs\n",
    "        output, h = net(inputs, h)\n",
    "        # calculate loss\n",
    "        test_loss = criterion(output, labels.to(torch.int64))\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.view_as(pred))\n",
    "        temp_label = labels.view_as(pred).tolist()\n",
    "        temp_pred = pred.tolist()\n",
    "        label_list.extend(temp_label)\n",
    "        predict_list.extend(temp_pred)\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "    # -- stats! -- ##\n",
    "    # avg test loss\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "    # accuracy over all test data\n",
    "    test_acc = num_correct / len(test_loader.dataset)\n",
    "    print(\"Test accuracy: {:.3f}\".format(test_acc))\n",
    "\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed Predicting Time:\", t1_stop - t1_start)\n",
    "\n",
    "    # -- stats! -- ##\n",
    "    print(accuracy_score(label_list, predict_list))\n",
    "    print(confusion_matrix(label_list, predict_list))\n",
    "    print(classification_report(label_list, predict_list))\n",
    "\n",
    "time_complexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}